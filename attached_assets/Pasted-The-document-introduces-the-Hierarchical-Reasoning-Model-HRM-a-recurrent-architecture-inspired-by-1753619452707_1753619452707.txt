The document introduces the Hierarchical Reasoning Model (HRM), a recurrent architecture inspired by the human brain's hierarchical and multi-timescale processing [1]. HRM is designed for complex reasoning tasks, achieving computational depth with training stability and efficiency, and operates via two modules: a high-level module for abstract planning and a low-level module for detailed computations [1].

HRM executes sequential reasoning tasks in a single forward pass without explicit intermediate supervision [1]. With 27 million parameters, it performs well on complex reasoning tasks using 1000 training samples, without pre-training or Chain-of-Thought (CoT) data [1]. HRM performs well on tasks like Sudoku and maze pathfinding, and it outperforms larger models on the Abstraction and Reasoning Corpus (ARC) [1].

The HRM maps an input vector x to an output prediction vector [1]. The input x is projected into a working representation Z by the input network [1]. At each timestep i, the L-module updates its state conditioned on its own previous state, the H- module’s current state, and the input representation [1]. The H-module only updates once per cycle using the L-module’s final state at the end of that cycle [1]. After V full cycles, a prediction is extracted from the hidden state of the H-module [1]. A halting mechanism determines whether the model should terminate or continue with an additional forward pass [1]. The document also details a deep supervision mechanism incorporated into HRM [1]. Given a data sample (x,y), multiple forward passes of the HRM model are run, each referred to as a segment [1]. At each segment m, a deep supervision step is applied, computing the next state and its associated output, computing the loss for the current segment, and updating parameters [1]. The hidden state is "detached" from the computation graph before being used as the input state for the next segment, effectively creating a 1-step approximation of the gradient of the recursive deep supervision process [1].

Sources:
[1] The Survey of Retrieval-Augmented Text Generation in Large - 2404.10981v2.pdf
[2] Hierarchical Reasoning Model - 2506.21734v2.pdf